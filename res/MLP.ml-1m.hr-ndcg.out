MLP arguments: Namespace(batch_size=256, dataset='ml-1m.hr-ndcg', epochs=20, layers='[64,32,16,8]', learner='adam', lr=0.001, num_neg=4, out=1, path='Data/', precisionK=500, recallK=300, recall_precision=False, reg_layers='[0,0,0,0]', topK=10, verbose=1) 
Load data done [25.4 s]. #user=6040, #item=3952, #train=994169, #test=6040
Init: HR = 0.0932, NDCG = 0.0413	 [164.3 s]
Iteration 0 [131.1 s]: HR = 0.6732, NDCG = 0.4119, loss = 0.3336 [1.2 s]
Iteration 1 [111.4 s]: HR = 0.7303, NDCG = 0.4584, loss = 0.2939 [1.2 s]
Iteration 2 [111.5 s]: HR = 0.7470, NDCG = 0.4789, loss = 0.2769 [1.2 s]
Iteration 3 [114.7 s]: HR = 0.7637, NDCG = 0.4980, loss = 0.2662 [1.2 s]
Iteration 4 [114.9 s]: HR = 0.7762, NDCG = 0.5084, loss = 0.2591 [1.3 s]
Iteration 5 [111.1 s]: HR = 0.7795, NDCG = 0.5193, loss = 0.2544 [1.2 s]
Iteration 6 [110.9 s]: HR = 0.7889, NDCG = 0.5244, loss = 0.2505 [1.3 s]
Iteration 7 [111.4 s]: HR = 0.7876, NDCG = 0.5278, loss = 0.2477 [1.2 s]
Iteration 8 [110.4 s]: HR = 0.7957, NDCG = 0.5312, loss = 0.2452 [1.3 s]
Iteration 9 [110.7 s]: HR = 0.7990, NDCG = 0.5324, loss = 0.2429 [1.2 s]
Iteration 10 [110.6 s]: HR = 0.7952, NDCG = 0.5343, loss = 0.2411 [1.5 s]
Iteration 11 [110.2 s]: HR = 0.7939, NDCG = 0.5315, loss = 0.2396 [1.2 s]
Iteration 12 [110.4 s]: HR = 0.7939, NDCG = 0.5349, loss = 0.2382 [1.3 s]
Iteration 13 [111.9 s]: HR = 0.7925, NDCG = 0.5311, loss = 0.2367 [1.2 s]
Iteration 14 [114.7 s]: HR = 0.7942, NDCG = 0.5334, loss = 0.2356 [1.3 s]
Iteration 15 [112.8 s]: HR = 0.7974, NDCG = 0.5352, loss = 0.2347 [1.2 s]
Iteration 16 [110.6 s]: HR = 0.7970, NDCG = 0.5325, loss = 0.2338 [1.2 s]
Iteration 17 [110.6 s]: HR = 0.7964, NDCG = 0.5351, loss = 0.2331 [1.3 s]
Iteration 18 [110.5 s]: HR = 0.7978, NDCG = 0.5355, loss = 0.2321 [1.2 s]
Iteration 19 [110.3 s]: HR = 0.7985, NDCG = 0.5360, loss = 0.2312 [1.2 s]
End. Best Iteration 9:  HR = 0.7990, NDCG = 0.5324. 
The best MLP model is saved to Pretrain/ml-1m.hr-ndcg_MLP_[64,32,16,8]_1596295790.h5
